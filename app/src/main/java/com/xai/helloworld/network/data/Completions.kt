package com.xai.helloworld.network.data

import kotlinx.serialization.SerialName
import kotlinx.serialization.Serializable

/**
 * Completions request for a given prompt. See [com.xai.helloworld.network.XAiApi.getCompletions]
 *
 * @property prompt Prompt for the completion
 * @property bestOf ***Not functional Yet*** Generate multiple completions and return the
 * top-scoring one.
 * @property echo Option to include the original prompt in the response along with generated
 * completion.
 * @property frequencyPenalty Number between -2.0 and 2.0. Positive values penalize new tokens based
 * on their existing frequency in the text so far, decreasing the model's likelihood to repeat the
 * same line verbatim.
 * @property logitBias Accepts a JSON object that maps tokens to an associated bias value from -100
 * to 100. You can use this tokenizer tool to convert text to token IDs. Mathematically, the bias is
 * added to the logits generated by the model prior to sampling. The exact effect will vary per
 * model, but values between -1 and 1 should decrease or increase likelihood of selection; values
 * like -100 or 100 should result in a ban or exclusive selection of the relevant token.
 * @property logprobs Include the log probabilities on the `logprobs` most likely output tokens, as
 * well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most
 * likely tokens. The API will always return the logprob of the sampled token, so there may be up to
 * `logprobs+1` elements in the response. ***Should this be Int?***
 * @property maxTokens Limits the number of tokens that can be produced in the output. Ensure the
 * sum of prompt tokens and `max_tokens` does not exceed the model's context limit.
 * @property model Specifies the model to be used for the request.
 * @property n Determines how many completion sequences to produce for each prompt. Be cautious with
 * its use due to high token consumption; adjust `max_tokens` and stop sequences accordingly.
 * @property presencePenalty Number between -2.0 and 2.0. Positive values penalize new tokens based
 * on whether they appear in the text so far, increasing the model's likelihood to talk about new
 * topics.
 * @property seed If specified, our system will make a best effort to sample deterministically, such
 * that repeated requests with the same seed and parameters should return the same result.
 * Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter
 * to monitor changes in the backend.
 * @property stop Up to 4 sequences where the API will stop generating further tokens. The returned
 * text will not contain the stop sequence.
 * @property stream Whether to stream back partial progress. If set, tokens will be sent as
 * data-only server-sent events as they become available, with the stream terminated by a
 * `data: [DONE]` message.
 * @property streamOptions Options for streaming response. Only set this when you set stream: true.
 * See [StreamOptions].
 * @property suffix Optional string to append after the generated text.
 * @property temperature What sampling temperature to use, between 0 and 2. Higher values like 0.8
 * will make the output more random, while lower values like 0.2 will make it more focused and
 * deterministic. We generally recommend altering this or `top_p` but not both.
 * @property topP An alternative to sampling with temperature, called nucleus sampling, where the
 * model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the
 * tokens comprising the top 10% probability mass are considered. We generally recommend altering
 * this or temperature but not both.
 * @property user A unique identifier representing your end-user, which can help xAI to monitor and
 * detect abuse.
 */

@Serializable
data class CompletionsRequest(
    val prompt: String,
    @SerialName("best_of")
    val bestOf: Int? = null,
    val echo: Boolean? = null,
    @SerialName("frequency_penalty")
    val frequencyPenalty: Double? = null,
    @SerialName("logit_bias")
    val logitBias: Map<String, Int>? = null,
    val logprobs: Int? = null,
    @SerialName("max_tokens")
    val maxTokens: Int? = null,
    val model: String = DEFAULT_MODEL,
    val n: Int? = null,
    @SerialName("presence_penalty")
    val presencePenalty: Double? = null,
    val seed: Int? = null,
    val stop: List<String>? = null,
    val stream: Boolean? = null,
    @SerialName("stream_options")
    val streamOptions: StreamOptions? = null,
    val suffix: String? = null,
    val temperature: Double? = null,
    @SerialName("top_p")
    val topP: Double? = null,
    val user: String? = null,
) {
    init {
        if (bestOf != null && n != null) {
            require(bestOf > n) {"bestOf > n but is ${bestOf} > ${n}"}
        }
        if (frequencyPenalty != null) {
            require(frequencyPenalty in -2.0..2.0) {
                "frequencyPenalty must be between -2.0 and 2.0 but is $frequencyPenalty"
            }
        }
        logitBias?.forEach { (token, bias) ->
            require(bias in -100..100) {
                "logit bias must be between -100 and 100 but is $bias for token $token"
            }
        }
        if (logprobs != null) {
            require(logprobs in 1..5) {
                "logprobs must be between 1 and 5 but is $logprobs"
            }
        }
        if (presencePenalty != null) {
            require(presencePenalty in -2.0..2.0) {
                "presencePenalty must be between -2.0 and 2.0 but is $presencePenalty"
            }
        }
        if (stop != null) {
            require(stop.size <= 4) {
                "stop may have a maximum of 4 sequences but has ${stop.size} sequences"
            }
        }
        if (streamOptions != null) {
            require(stream == true) {"If streamOptions is set, stream must be true"}
        }
        if (temperature != null) {
            require(temperature in 0.0..2.0) {
                "temperature must be between 0.0 and 2.0 but is $temperature"
            }
        }
    }
}

/**
 * Stream options. See [CompletionsRequest.streamOptions]
 *
 * @property includeUsage If set, an additional chunk will be streamed before the data: [DONE]
 * message. The usage field on this chunk shows the token usage statistics for the entire request,
 * and the choices field will always be an empty array. All other chunks will also include a usage
 * field, but with a null value.
 */
@Serializable
data class StreamOptions(
    @SerialName("include_usage")
    val includeUsage: Boolean? = null,
)

/**
 * Completions response. See [com.xai.helloworld.network.XAiApi.getCompletions]
 *
 * @property id A unique identifier for the completion.
 * @property choices The list of completion choices the model generated for the input prompt. See
 * [Choice]
 * @property created The Unix timestamp (in seconds) of when the completion was created.
 * @property model The model used for completion.
 * @property systemFingerprint This fingerprint represents the backend configuration that the model
 * runs with. Can be used in conjunction with the seed request parameter to understand when backend
 * changes have been made that might impact determinism.
 * @property objectX The object type, which is always "text_completion"
 * @property usage Usage statistics for the completion request. See [Usage]
 */
@Serializable
data class CompletionsResponse(
    val id: String,
    val choices: List<Choice>,
    val created: Long,
    val model: String,
    @SerialName("system_fingerprint")
    val systemFingerprint: String,
    @SerialName("object")
    val objectX: String,
    val usage: Usage,
)

/**
 * A choice the model generated for the input prompt.
 *
 * @property finishReason The reason the model stopped generating tokens. This will be `stop` if the
 * model hit a natural stop point or a provided stop sequence, `length` if the maximum number of
 * tokens specified in the request was reached, or `content_filter` if content was omitted due to a
 * flag from our content filters.
 * @property index index of this potential response in the list
 * @property text a completion response for the prompt
 */
@Serializable
data class Choice(
    @SerialName("finish_reason")
    val finishReason: String,
    val index: Int,
    val text: String,
)

/**
 * Usage statistics for the completion request. See [CompletionsResponse.usage]
 *
 * @property completionTokens Number of tokens in the generated completion.
 * @property promptTokens Number of tokens in the prompt.
 * @property totalTokens Total number of tokens used in the request (prompt + completion).
 * @property promptTokensDetails Breakdown of tokens used in the prompt. See [PromptTokensDetails]
 * @constructor Create empty Usage
 */
@Serializable
data class Usage(
    @SerialName("completion_tokens")
    val completionTokens: Int,
    @SerialName("prompt_tokens")
    val promptTokens: Int,
    @SerialName("total_tokens")
    val totalTokens: Int,
    @SerialName("prompt_tokens_details")
    val promptTokensDetails: PromptTokensDetails,
)

/**
 * Breakdown of tokens used in the prompt.
 *
 * @property audioTokens Audio input tokens present in the prompt.
 * @property cachedTokens Cached tokens present in the prompt.
 * @property imageTokens
 * @property textTokens
 * @constructor Create empty Prompt tokens details
 */
@Serializable
data class PromptTokensDetails(
    @SerialName("audio_tokens")
    val audioTokens: Int,
    @SerialName("cached_tokens")
    val cachedTokens: Int,
    @SerialName("image_tokens")
    val imageTokens: Int,
    @SerialName("text_tokens")
    val textTokens: Int
)

